{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+pC/GCNR4n+YjzwNXUn3r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anthonyendara123/BASE_LABS_TEST/blob/main/BIBITOR_BASE_LABS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start of the setup and installation process for the Spark environment with SQLite support\n",
        "\n",
        "This section performs the necessary installation and configuration to run Apache Spark in this working environment (Google Colab or similar), including:\n",
        "\n",
        "Installation of OpenJDK 8, required to run Spark.\n",
        "\n",
        "Extraction of the Spark binary package (version 3.2.3 with Hadoop 3.2).\n",
        "\n",
        "Download of the JDBC driver for SQLite (sqlite-jdbc-3.34.0.jar), necessary for Spark to read and write SQLite tables via JDBC.\n",
        "\n",
        "Configuration of the environment variables JAVA_HOME and SPARK_HOME so that Spark and Java work correctly.\n",
        "\n",
        "Initialization of findspark to integrate Spark with the Python environment.\n",
        "\n",
        "Creation of the Spark session with the configuration to load the SQLite JDBC driver.\n",
        "\n",
        "This configuration is essential to be able to work with Spark and SQLite together, since the goal of the technical test is to demonstrate skills in integrating data from SQLite databases in a Spark environment, processing it, and analyzing it efficiently.\n",
        "\n"
      ],
      "metadata": {
        "id": "hBH3HuHNi905"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "K9VOssMY0G_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcQ-50SidqH"
      },
      "outputs": [],
      "source": [
        "# install JAVA 8 SDK\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the Spark version\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!wget https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asED-PTOjOPy",
        "outputId": "98f9676f-a3a9-4613-ba54-cfbd3441b69d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-08 21:33:07--  https://repo1.maven.org/maven2/org/xerial/sqlite-jdbc/3.34.0/sqlite-jdbc-3.34.0.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7296329 (7.0M) [application/java-archive]\n",
            "Saving to: â€˜sqlite-jdbc-3.34.0.jar.2â€™\n",
            "\n",
            "\rsqlite-jdbc-3.34.0.   0%[                    ]       0  --.-KB/s               \rsqlite-jdbc-3.34.0. 100%[===================>]   6.96M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-07-08 21:33:07 (81.7 MB/s) - â€˜sqlite-jdbc-3.34.0.jar.2â€™ saved [7296329/7296329]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.3-bin-hadoop3.2\"\n",
        "\n",
        "# download findspark library\n",
        "!pip install -q findspark\n",
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "KijBOzGljbvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Leer SQLite\") \\\n",
        "    .config(\"spark.jars\", \"sqlite-jdbc-3.34.0.jar\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "_SjSngnFjmeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The configuration and installation of Spark and the SQLite environment has completed successfully.\n",
        "Now, the environment is ready, and we begin the main process of loading and processing data."
      ],
      "metadata": {
        "id": "OOY2phv915RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import gdown\n",
        "import zipfile\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import time, datetime\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import format_number\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "ENTITY = \"BIBITOR\"\n",
        "base_folder = \"/content/final_reports\"\n",
        "db_path = \"/content/mi_base.db\"\n",
        "\n",
        "###################################################\n",
        "#                                                 #\n",
        "# == 1. DEFINICIÃ“N DE FUNCIONES PERSONALIZADAS == #\n",
        "#                                                 #\n",
        "###################################################\n",
        "def extract_id(google_drive_url):\n",
        "    \"\"\"\n",
        "    Extracts the unique ID from a Google Drive link.\n",
        "\n",
        "    This function takes a Google Drive URL and searches for a specific pattern\n",
        "    that identifies the file ID (the part following '/d/' in the URL).\n",
        "    If the ID is found, it returns it as a string; otherwise, it returns None.\n",
        "    \"\"\"\n",
        "    match = re.search(r'/d/([a-zA-Z0-9_-]+)', google_drive_url)\n",
        "    return match.group(1) if match else None\n",
        "\n",
        "def download_files(wget1, wget2, wget3, wget4):\n",
        "    \"\"\"\n",
        "    Executes a series of download commands using wget.\n",
        "\n",
        "    It takes four commands (wget) as arguments and executes them one by one.\n",
        "    For each command:\n",
        "    - Prints the number of the download being executed.\n",
        "    - Executes the command and displays the output line by line.\n",
        "    - Captures and displays any errors that occur during the download.\n",
        "    At the end, it indicates that all downloads were completed.\n",
        "    \"\"\"\n",
        "    comands = [wget1, wget2, wget3, wget4]\n",
        "    for i, cmd in enumerate(comands, 1):\n",
        "        try:\n",
        "            print(f\"\\n Running download {i}...\")\n",
        "            salida = get_ipython().getoutput(cmd)\n",
        "            for linea in salida:\n",
        "                print(linea)\n",
        "        except Exception as e:\n",
        "            print(f\" Download error {i}: {e}\")\n",
        "    print(\"âœ… Downloads completed.\\n\")\n",
        "\n",
        "def download_and_extract_zip(url, nombre_zip=\"archivo.zip\", destination_folder=\"archivos\"):\n",
        "    \"\"\"\n",
        "    Downloads a ZIP file from a Google Drive link and unzips it.\n",
        "\n",
        "    Steps:\n",
        "    1. Extracts the file ID from the Google Drive URL.\n",
        "    2. Downloads the ZIP file using 'gdown' and saves it with the specified name.\n",
        "    3. Creates the destination folder if it doesn't exist.\n",
        "    4. Unzips the ZIP file into the destination folder.\n",
        "    5. Lists the extracted files in the console.\n",
        "\n",
        "    Parameters:\n",
        "    - url: Google Drive link to the ZIP file.\n",
        "    - zip_name: Local name to save the ZIP file (default \"file.zip\").\n",
        "    - destination_folder: Folder where the files are extracted (default \"files\").\n",
        "\n",
        "    Handles exceptions to display errors during download or extraction.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_id = extract_id(url)\n",
        "        if not file_id:\n",
        "            print(f\" Could not extract link ID: {url}\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n Downloading ZIP from: {url}\")\n",
        "        gdown.download(f\"https://drive.google.com/uc?id={file_id}\", nombre_zip, quiet=False)\n",
        "\n",
        "        print(f\" Unzipping into: {destination_folder}/\")\n",
        "        os.makedirs(destination_folder, exist_ok=True)\n",
        "        with zipfile.ZipFile(nombre_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(destination_folder)\n",
        "\n",
        "        print(f\" Extracted content: \")\n",
        "        for f in os.listdir(destination_folder):\n",
        "            print(\"  -\", f)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error downloading or unzipping the ZIP: {e}\")\n",
        "\n",
        "def load_tables_from_result(results_tbl, db_path):\n",
        "    \"\"\"\n",
        "    Loads tables from an SQLite database into Spark and registers them as temporary views.\n",
        "\n",
        "    Parameters:\n",
        "    - results_tbl: List of tuples with table names, e.g. [('table1',), ('table2',), ...]\n",
        "    - db_path: Path to the SQLite database file.\n",
        "\n",
        "    Process:\n",
        "    1. Extracts the table names from the list of tuples.\n",
        "    2. Loads each table into a Spark DataFrame using JDBC.\n",
        "    3. Registers each DataFrame as a temporary view with the table name.\n",
        "    4. Saves the DataFrames in a dictionary with the table as the key.\n",
        "\n",
        "    Returns:\n",
        "    - A dictionary where the key is the table name and the value is the loaded DataFrame.\n",
        "    \"\"\"\n",
        "    dfs = {}\n",
        "\n",
        "    # Extract table names from [('table1',), ('table2',)...]\n",
        "    table_names = [t[0] for t in results_tbl]\n",
        "\n",
        "    for name in table_names:\n",
        "        df = spark.read.format(\"jdbc\") \\\n",
        "            .option(\"url\", f\"jdbc:sqlite:{db_path}\") \\\n",
        "            .option(\"dbtable\", name) \\\n",
        "            .option(\"driver\", \"org.sqlite.JDBC\") \\\n",
        "            .load()\n",
        "\n",
        "        df.createOrReplaceTempView(name)\n",
        "        dfs[name] = df\n",
        "        print(f\" Table '{name}' loaded and registered.\")\n",
        "\n",
        "    return dfs\n",
        "\n",
        "def save_csv(df, name):\n",
        "    \"\"\"\n",
        "    Saves a PySpark DataFrame as a single CSV file within a defined base folder.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pyspark.sql.DataFrame\n",
        "        DataFrame to be saved in CSV format.\n",
        "    name : str\n",
        "        Base name for the temporary folder and the final CSV file.\n",
        "\n",
        "    Process:\n",
        "    --------\n",
        "    1. Builds a temporary path by combining the base folder and the provided name.\n",
        "    2. Writes the DataFrame in CSV format to this temporary path, consolidating the data into a single file\n",
        "       using coalesce(1). Any previous content is overwritten.\n",
        "    3. Searches for the generated CSV file within the temporary folder (with prefix 'part-').\n",
        "    4. Moves this CSV file to the final path named \"{name}.csv\" within the base folder.\n",
        "    5. Deletes the temporary folder used for writing, leaving only the final CSV file.\n",
        "\n",
        "    This ensures the result is a single, properly named CSV file, avoiding partitioned file fragments and\n",
        "    keeping the storage structure organized.\n",
        "    \"\"\"\n",
        "    temp_path = os.path.join(base_folder, name)\n",
        "    df.coalesce(1).write.csv(temp_path, header=True, mode=\"overwrite\")\n",
        "    csv_file = glob(os.path.join(temp_path, \"part-*.csv\"))[0]\n",
        "    final_path = os.path.join(base_folder, f\"{name}.csv\")\n",
        "    shutil.move(csv_file, final_path)\n",
        "    shutil.rmtree(temp_path)\n",
        "\n",
        "################################################\n",
        "#                                              #\n",
        "# ==     2. process setting block           == #\n",
        "#                                              #\n",
        "################################################\n",
        "start_time = time.time()\n",
        "print(f\"Start of process for ENTITY {ENTITY}: {datetime.datetime.fromtimestamp(start_time)}\")\n",
        "\n",
        "try:\n",
        "    ENV_VALUE = 1\n",
        "    table = \"params\" if ENV_VALUE == 1 else \"params_des\"\n",
        "    print(f\"== Selected environment is: {ENV_VALUE} ==\")\n",
        "    print(f\"== Using table: {table} ==\")\n",
        "\n",
        "    print(\"== Establishing connection with SQLite database and creating a cursor to execute queries ==\")\n",
        "    conn = sqlite3.connect('mi_base.db')\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    print(\"== Creating the specified table if it does not exist, with columns ENTITY, PARAMETRO, and VALOR ==\")\n",
        "    cursor.execute(f'''\n",
        "    CREATE TABLE IF NOT EXISTS {table} (\n",
        "        ENTITY TEXT,\n",
        "        PARAMETRO TEXT,\n",
        "        VALOR TEXT\n",
        "    )''')\n",
        "\n",
        "    print(f\"== Deleting all rows from the table where the ENTITY column equals the variable {ENTITY}, and committing changes ==\")\n",
        "    cursor.execute(f\"DELETE FROM {table} WHERE ENTITY = ?\", (ENTITY,))\n",
        "    conn.commit()\n",
        "\n",
        "    print(\"== Inserting list of tuples with pairs (variable name, value) into the inserts variable ==\")\n",
        "    print(\"== Variables starting with VAR_GET_ contain commands or URLs to download CSV files ==\")\n",
        "    print(\"== Variables starting with VAR_tbl_ contain the names of tables where those data will be stored ==\")\n",
        "    inserts = [\n",
        "        (\"VAR_GET_2017PurchasePricesDec\", 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d\" -O 2017PurchasePricesDec.csv'),\n",
        "        (\"VAR_GET_BegInvFINAL12312016\", 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK\" -O BegInvFINAL12312016.csv'),\n",
        "        (\"VAR_GET_EndInvFINAL12312016\", 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB\" -O EndInvFINAL12312016.csv'),\n",
        "        (\"VAR_GET_InvoicePurchases12312016\", 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7\" -O InvoicePurchases12312016.csv'),\n",
        "        (\"VAR_GET_PurchasesFINAL12312016\", 'https://drive.google.com/file/d/1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd/view?usp=drive_link'),\n",
        "        (\"VAR_GET_SalesFINAL12312016\", 'https://drive.google.com/file/d/1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY/view?usp=sharing'),\n",
        "        (\"VAR_tbl_2017PurchasePricesDec\", 'PricingPurchasesDec'),\n",
        "        (\"VAR_tbl_BegInvFINAL12312016\", 'BegInvDec'),\n",
        "        (\"VAR_tbl_EndInvFINAL12312016\", 'EndInvDec'),\n",
        "        (\"VAR_tbl_InvoicePurchases12312016\", 'VendorInvoicesDec'),\n",
        "        (\"VAR_tbl_PurchasesFINAL12312016\", 'PurchasesDec'),\n",
        "        (\"VAR_tbl_SalesFINAL12312016\", 'SalesDec'),\n",
        "        (\"ETAPA\", \"1\"),\n",
        "    ]\n",
        "\n",
        "    print(f\"== Inserting parameters and values into SQLite table for specified ENTITY: {ENTITY} and committing changes ==\")\n",
        "    for param, value in inserts:\n",
        "        print(f\"Inserting -> ENTITY: {ENTITY}, PARAMETRO: {param}, VALOR: {value}\")\n",
        "        cursor.execute(f\"INSERT INTO {table} (ENTITY, PARAMETRO, VALOR) VALUES (?, ?, ?)\", (ENTITY, param, value))\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"== Getting the value of ETAPA for ENTITY: {ENTITY} and converting to integer ==\")\n",
        "    cursor.execute(f\"SELECT VALOR FROM {table} WHERE ENTITY = ? AND PARAMETRO = 'ETAPA'\", (ENTITY,))\n",
        "    row = cursor.fetchone()\n",
        "    if row:\n",
        "        stage = int(row[0])  # convert to integer\n",
        "        print(f\"ðŸ”¹ ETAPA found: {stage} (type: {type(stage).__name__})\")\n",
        "    else:\n",
        "        raise ValueError(\"ETAPA value not found for ENTITY.\")\n",
        "\n",
        "    te_time = time.time()\n",
        "    print(f\"End of block 2 (PROCESS ENVIRONMENT SETUP) \", datetime.datetime.fromtimestamp(te_time))\n",
        "    elapsed_time = te_time - start_time\n",
        "    print(f\"Block 2 finished in : {elapsed_time:.2f} seconds\")\n",
        "\n",
        "    ################################################\n",
        "    #                                              #\n",
        "    # ==    3. DATA SOURCES LOAD BLOCK          == #\n",
        "    #                                              #\n",
        "    ################################################\n",
        "\n",
        "    if stage == 1:\n",
        "        ts_time = time.time()\n",
        "        print(f\"Start of block 3 (LOAD CSV DATA SOURCES) STAGE {stage}\", datetime.datetime.fromtimestamp(ts_time))\n",
        "        print(f\"== Query and extract 6 VAR_GET_ values for ENTITY: {ENTITY} ==\")\n",
        "        sql = f\"SELECT VALOR FROM {table} WHERE ENTITY = ? AND PARAMETRO LIKE 'VAR_GET_%'\"\n",
        "        cursor.execute(sql, (ENTITY,))\n",
        "        results = cursor.fetchall()\n",
        "\n",
        "        if len(results) == 6:\n",
        "            val_1, val_2, val_3, val_4, val_5, val_6 = [row[0] for row in results]\n",
        "        else:\n",
        "            raise ValueError(f\"Expected 6 values but found {len(results)}.\")\n",
        "\n",
        "        print(\"== Download files using the first 4 obtained URLs/commands ==\")\n",
        "        download_files(val_1, val_2, val_3, val_4)\n",
        "\n",
        "        print(\"== Download and extract ZIPs from the specified URLs ==\")\n",
        "        download_and_extract_zip(val_6)\n",
        "        download_and_extract_zip(val_5)\n",
        "\n",
        "        print(\"== Read CSV files into pandas DataFrames ==\")\n",
        "        df1 = pd.read_csv(\"/content/2017PurchasePricesDec.csv\")\n",
        "        df2 = pd.read_csv(\"archivos/SalesFINAL12312016.csv\")\n",
        "        df3 = pd.read_csv(\"/content/BegInvFINAL12312016.csv\")\n",
        "        df4 = pd.read_csv(\"/content/EndInvFINAL12312016.csv\")\n",
        "        df5 = pd.read_csv(\"/content/InvoicePurchases12312016.csv\")\n",
        "        df6 = pd.read_csv(\"/content/archivos/PurchasesFINAL12312016.csv\")\n",
        "\n",
        "        print(\"== Info df1 (2017PurchasePricesDec) ==\")\n",
        "        df1.info()\n",
        "        print(\"\\n== Info df2 (SalesFINAL12312016) ==\")\n",
        "        df2.info()\n",
        "        print(\"\\n== Info df3 (BegInvFINAL12312016) ==\")\n",
        "        df3.info()\n",
        "        print(\"\\n== Info df4 (EndInvFINAL12312016) ==\")\n",
        "        df4.info()\n",
        "        print(\"\\n== Info df5 (InvoicePurchases12312016) ==\")\n",
        "        df5.info()\n",
        "        print(\"\\n== Info df6 (PurchasesFINAL12312016) ==\")\n",
        "        df6.info()\n",
        "\n",
        "        print(\"\\n== Extract VAR_tbl_ values and assign to variables ==\")\n",
        "        sql_tbl = f\"SELECT VALOR FROM {table} WHERE ENTITY = ? AND PARAMETRO LIKE 'VAR_tbl_%'\"\n",
        "        cursor.execute(sql_tbl, (ENTITY,))\n",
        "        results_tbl = cursor.fetchall()\n",
        "\n",
        "        if len(results_tbl) == 6:\n",
        "            tbl_1, tbl_2, tbl_3, tbl_4, tbl_5, tbl_6 = [row[0] for row in results_tbl]\n",
        "        else:\n",
        "            raise ValueError(f\"Expected 6 VAR_tbl_ values but found {len(results_tbl)}.\")\n",
        "\n",
        "        print(\"== Create dictionary of table names and DataFrames ==\")\n",
        "        tables_df = {\n",
        "            tbl_1: df1,\n",
        "            tbl_2: df2,\n",
        "            tbl_3: df3,\n",
        "            tbl_4: df4,\n",
        "            tbl_5: df5,\n",
        "            tbl_6: df6\n",
        "        }\n",
        "\n",
        "        print(\"== Save DataFrames to SQLite if not empty ==\")\n",
        "        for table_name, df in tables_df.items():\n",
        "            if not df.empty:\n",
        "                print(f\"Saving {table_name} ({len(df)} rows)...\")\n",
        "                df.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
        "            else:\n",
        "                print(f\"{table_name} is empty. Skipping save.\")\n",
        "\n",
        "        print(f\"Process {ENTITY} completed successfully\")\n",
        "        print(f\"Updating stage for {ENTITY} to 2\")\n",
        "        stage += 1\n",
        "        sql_update = f\"\"\"\n",
        "        UPDATE {table}\n",
        "        SET VALOR = ?\n",
        "        WHERE ENTITY = ? AND PARAMETRO = 'ETAPA'\n",
        "        \"\"\"\n",
        "\n",
        "        cursor.execute(sql_update, (str(stage), ENTITY))\n",
        "        conn.commit()\n",
        "\n",
        "        cursor.execute(f\"SELECT * FROM {table}\")\n",
        "        rows = cursor.fetchall()\n",
        "\n",
        "        for row in rows:\n",
        "            print(row)\n",
        "\n",
        "        te_time = time.time()\n",
        "        print(f\"End of block 3 (LOAD CSV DATA SOURCES) STAGE 1\", datetime.datetime.fromtimestamp(te_time))\n",
        "        elapsed_time_load = te_time - ts_time\n",
        "        print(f\"(LOAD CSV DATA SOURCES) STAGE 1 completed in:\", datetime.datetime.fromtimestamp(elapsed_time_load))\n",
        "\n",
        "    ##############################################\n",
        "    #                                            #\n",
        "    # ==    4. LOADING TABLES INTO DATAFRAMES  == #\n",
        "    #                                            #\n",
        "    ##############################################\n",
        "    if stage == 2:\n",
        "        ts_time = time.time()\n",
        "        print(f\"Start of block 4 (LOAD TABLES TO DATAFRAMES AND BUSINESS LOGIC) STAGE {stage}\", datetime.datetime.fromtimestamp(ts_time))\n",
        "\n",
        "        print(\"== Calling function load_tables_from_result to load tables into dfs and then save them into their corresponding dataframes ==\")\n",
        "        dfs = load_tables_from_result(results_tbl, db_path)\n",
        "        df_SalesDec = dfs[\"SalesDec\"]\n",
        "        df_PricingPurchasesDec = dfs[\"PricingPurchasesDec\"]\n",
        "        df_BegInvDec = dfs[\"BegInvDec\"]\n",
        "        df_EndInvDec = dfs[\"EndInvDec\"]\n",
        "        df_VendorInvoicesDec = dfs[\"VendorInvoicesDec\"]\n",
        "        df_PurchasesDec = dfs[\"PurchasesDec\"]\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_SalesDec ==\")\n",
        "        df_SalesDec.printSchema()\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_PricingPurchasesDec ==\")\n",
        "        df_PricingPurchasesDec.printSchema()\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_BegInvDec and caching it because it's heavy and we need it in memory ==\")\n",
        "        df_BegInvDec.cache().printSchema()\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_EndInvDec ==\")\n",
        "        df_EndInvDec.printSchema()\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_VendorInvoicesDec ==\")\n",
        "        df_VendorInvoicesDec.printSchema()\n",
        "\n",
        "        print(\"== Performing printSchema() on dataframe df_PurchasesDec ==\")\n",
        "        df_PurchasesDec.printSchema()\n",
        "\n",
        "        #######################################\n",
        "        #                                     #\n",
        "        # ==    5. BUSINESS LOGIC          == #\n",
        "        #                                     #\n",
        "        #######################################\n",
        "        print(\"== Selecting necessary columns from Pricing dataframe ==\")\n",
        "        df_cost = df_PricingPurchasesDec.select(\"Brand\", \"Description\", \"Size\", \"PurchasePrice\")\n",
        "\n",
        "        print(\"== Joining sales dataframe with cost info from pricing dataframe ==\")\n",
        "        df_sales_with_cost = df_BegInvDec.join(\n",
        "            df_cost,\n",
        "            on=[\"Brand\", \"Description\", \"Size\"],\n",
        "            how=\"left\"\n",
        "        )\n",
        "\n",
        "        print(\"== Calculating Profit ($), Revenue ($), and Margin (%) ==\")\n",
        "        df_sales_profit = df_sales_with_cost.withColumn(\n",
        "            \"Profit\",\n",
        "            (F.col(\"SalesPrice\") - F.col(\"PurchasePrice\")) * F.col(\"SalesQuantity\")\n",
        "        ).withColumn(\n",
        "            \"Revenue\",\n",
        "            F.col(\"SalesPrice\") * F.col(\"SalesQuantity\")\n",
        "        ).withColumn(\n",
        "            \"Margin\",\n",
        "            F.when(F.col(\"Revenue\") != 0, (F.col(\"Profit\") / F.col(\"Revenue\")) * 100).otherwise(0)\n",
        "        )\n",
        "\n",
        "        print(\"== Getting Top 10 products by Profit ==\")\n",
        "        top10_products_profit = df_sales_profit.orderBy(F.desc(\"Profit\")).limit(10)\n",
        "        top10_products_profit.show()\n",
        "\n",
        "        print(\"== Getting Top 10 products by Margin ==\")\n",
        "        top10_products_margin = df_sales_profit.orderBy(F.desc(\"Margin\")).limit(10)\n",
        "        top10_products_margin.show()\n",
        "\n",
        "        print(\"== Aggregating Profit and Revenue by Brand ==\")\n",
        "        df_brands = df_sales_profit.groupBy(\"Brand\").agg(\n",
        "            F.sum(\"Profit\").alias(\"TotalProfit\"),\n",
        "            F.sum(\"Revenue\").alias(\"TotalRevenue\")\n",
        "        ).withColumn(\n",
        "            \"Margin\",\n",
        "            F.when(F.col(\"TotalRevenue\") != 0, (F.col(\"TotalProfit\") / F.col(\"TotalRevenue\")) * 100).otherwise(0)\n",
        "        )\n",
        "\n",
        "        print(\"== Getting Top 10 brands by Profit ==\")\n",
        "        top10_brands_profit = df_brands.orderBy(F.desc(\"TotalProfit\")).limit(10)\n",
        "        top10_brands_profit.show()\n",
        "\n",
        "        print(\"== Getting Top 10 brands by Margin ==\")\n",
        "        top10_brands_margin = df_brands.orderBy(F.desc(\"Margin\")).limit(10)\n",
        "        top10_brands_margin.show()\n",
        "\n",
        "        print(\"== Filtering products losing money (Profit < 0) ==\")\n",
        "        products_losing_money = df_sales_profit.filter(F.col(\"Profit\") < 0)\n",
        "        products_losing_money.show()\n",
        "\n",
        "        print(\"== Filtering brands losing money (TotalProfit < 0) ==\")\n",
        "        brands_losing_money = df_brands.filter(F.col(\"TotalProfit\") < 0)\n",
        "        brands_losing_money.show()\n",
        "\n",
        "        print(\"== Saving reports using save_csv function ==\")\n",
        "\n",
        "        te_time = time.time()\n",
        "        print(f\"End of blocks 4, 5 and 6 (LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 \", datetime.datetime.fromtimestamp(te_time))\n",
        "        elapsed_time_carga = te_time - ts_time\n",
        "        print(f\"(LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 completed in :\", datetime.datetime.fromtimestamp(elapsed_time_carga))\n",
        "\n",
        "        #################################################################\n",
        "        #                                                               #\n",
        "        # ==    6.Save resulting files from the queries               == #\n",
        "        #                                                               #\n",
        "        #################################################################\n",
        "        print(\"== We call the save_csv function to save the dfs in our path /content/final_reports/....... to be able to download them and use as sources ==\")\n",
        "        save_csv(top10_products_profit, \"Top10_Products_Profit\")\n",
        "        save_csv(top10_products_margin, \"Top10_Products_Margin\")\n",
        "\n",
        "        save_csv(top10_brands_profit, \"Top10_Brands_Profit\")\n",
        "        save_csv(top10_brands_margin, \"Top10_Brands_Margin\")\n",
        "\n",
        "        save_csv(products_losing_money, \"Products_Losing_Money\")\n",
        "        save_csv(brands_losing_money, \"Brands_Losing_Money\")\n",
        "\n",
        "        print(f\"End of blocks 4, 5 and 6 (LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 \", datetime.datetime.fromtimestamp(te_time))\n",
        "        elapsed_time_carga = te_time - ts_time\n",
        "        print(f\"(LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 completed in :\", datetime.datetime.fromtimestamp(elapsed_time_carga))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nGeneral process error: {e}\")\n",
        "\n",
        "finally:\n",
        "    print(\"== Closing SQLite connection if open ==\")\n",
        "    if 'conn' in locals():\n",
        "        conn.close()\n",
        "        print(\"SQLite connection closed.\")\n",
        "\n",
        "    print(\"== Releasing memory by deleting used variables ==\")\n",
        "    vars_to_delete = ['df1', 'df2', 'df3', 'df4', 'df5', 'df6',\n",
        "                      'valor_1', 'valor_2', 'valor_3', 'valor_4', 'valor_5', 'valor_6',\n",
        "                      'tablas_df', 'cursor', 'tabla', 'ENTITY']\n",
        "    for var in vars_to_delete:\n",
        "        if var in locals():\n",
        "            del locals()[var]\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"End of process:\", datetime.datetime.fromtimestamp(end_time))\n",
        "    print(f\"Process completed in {elapsed_time:.2f} seconds\")\n",
        "    print(\"== Closing spark session to free resources ==\")\n",
        "\n",
        "    #spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh-wmCBck4Ix",
        "outputId": "74cedc7f-b8da-47ef-8b7b-8a811a2cfe16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of process for ENTITY BIBITOR: 2025-07-08 21:33:25.482165\n",
            "== Selected environment is: 1 ==\n",
            "== Using table: params ==\n",
            "== Establishing connection with SQLite database and creating a cursor to execute queries ==\n",
            "== Creating the specified table if it does not exist, with columns ENTITY, PARAMETRO, and VALOR ==\n",
            "== Deleting all rows from the table where the ENTITY column equals the variable BIBITOR, and committing changes ==\n",
            "== Inserting list of tuples with pairs (variable name, value) into the inserts variable ==\n",
            "== Variables starting with VAR_GET_ contain commands or URLs to download CSV files ==\n",
            "== Variables starting with VAR_tbl_ contain the names of tables where those data will be stored ==\n",
            "== Inserting parameters and values into SQLite table for specified ENTITY: BIBITOR and committing changes ==\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_2017PurchasePricesDec, VALOR: wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d\" -O 2017PurchasePricesDec.csv\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_BegInvFINAL12312016, VALOR: wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK\" -O BegInvFINAL12312016.csv\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_EndInvFINAL12312016, VALOR: wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB\" -O EndInvFINAL12312016.csv\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_InvoicePurchases12312016, VALOR: wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7\" -O InvoicePurchases12312016.csv\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_PurchasesFINAL12312016, VALOR: https://drive.google.com/file/d/1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd/view?usp=drive_link\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_GET_SalesFINAL12312016, VALOR: https://drive.google.com/file/d/1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY/view?usp=sharing\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_2017PurchasePricesDec, VALOR: PricingPurchasesDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_BegInvFINAL12312016, VALOR: BegInvDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_EndInvFINAL12312016, VALOR: EndInvDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_InvoicePurchases12312016, VALOR: VendorInvoicesDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_PurchasesFINAL12312016, VALOR: PurchasesDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: VAR_tbl_SalesFINAL12312016, VALOR: SalesDec\n",
            "Inserting -> ENTITY: BIBITOR, PARAMETRO: ETAPA, VALOR: 1\n",
            "== Getting the value of ETAPA for ENTITY: BIBITOR and converting to integer ==\n",
            "ðŸ”¹ ETAPA found: 1 (type: int)\n",
            "End of block 2 (PROCESS ENVIRONMENT SETUP)  2025-07-08 21:33:25.503362\n",
            "Block 2 finished in : 0.02 seconds\n",
            "Start of block 3 (LOAD CSV DATA SOURCES) STAGE 1 2025-07-08 21:33:25.503412\n",
            "== Query and extract 6 VAR_GET_ values for ENTITY: BIBITOR ==\n",
            "== Download files using the first 4 obtained URLs/commands ==\n",
            "\n",
            " Running download 1...\n",
            "--2025-07-08 21:33:25--  https://drive.google.com/uc?export=download&id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.102, 142.250.107.139, 142.250.107.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d&export=download [following]\n",
            "--2025-07-08 21:33:25--  https://drive.usercontent.google.com/download?id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.107.132, 2607:f8b0:400e:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.107.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1157603 (1.1M) [application/octet-stream]\n",
            "Saving to: â€˜2017PurchasePricesDec.csvâ€™\n",
            "\n",
            "\n",
            "          2017Purch   0%[                    ]       0  --.-KB/s               \n",
            "2017PurchasePricesD 100%[===================>]   1.10M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-07-08 21:33:27 (35.9 MB/s) - â€˜2017PurchasePricesDec.csvâ€™ saved [1157603/1157603]\n",
            "\n",
            "\n",
            " Running download 2...\n",
            "--2025-07-08 21:33:27--  https://drive.google.com/uc?export=download&id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.102, 142.250.107.139, 142.250.107.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK&export=download [following]\n",
            "--2025-07-08 21:33:27--  https://drive.usercontent.google.com/download?id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.107.132, 2607:f8b0:400e:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.107.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19306233 (18M) [application/octet-stream]\n",
            "Saving to: â€˜BegInvFINAL12312016.csvâ€™\n",
            "\n",
            "\n",
            "BegInvFINAL12312016   0%[                    ]       0  --.-KB/s               \n",
            "BegInvFINAL12312016  61%[===========>        ]  11.32M  56.6MB/s               \n",
            "BegInvFINAL12312016 100%[===================>]  18.41M  56.1MB/s    in 0.3s    \n",
            "\n",
            "2025-07-08 21:33:32 (56.1 MB/s) - â€˜BegInvFINAL12312016.csvâ€™ saved [19306233/19306233]\n",
            "\n",
            "\n",
            " Running download 3...\n",
            "--2025-07-08 21:33:33--  https://drive.google.com/uc?export=download&id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.102, 142.250.107.139, 142.250.107.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB&export=download [following]\n",
            "--2025-07-08 21:33:33--  https://drive.usercontent.google.com/download?id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.107.132, 2607:f8b0:400e:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.107.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21004265 (20M) [application/octet-stream]\n",
            "Saving to: â€˜EndInvFINAL12312016.csvâ€™\n",
            "\n",
            "\n",
            "EndInvFINAL12312016   0%[                    ]       0  --.-KB/s               \n",
            "EndInvFINAL12312016 100%[===================>]  20.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-07-08 21:33:35 (157 MB/s) - â€˜EndInvFINAL12312016.csvâ€™ saved [21004265/21004265]\n",
            "\n",
            "\n",
            " Running download 4...\n",
            "--2025-07-08 21:33:35--  https://drive.google.com/uc?export=download&id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.102, 142.250.107.139, 142.250.107.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7&export=download [following]\n",
            "--2025-07-08 21:33:35--  https://drive.usercontent.google.com/download?id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.107.132, 2607:f8b0:400e:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.107.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 591021 (577K) [application/octet-stream]\n",
            "Saving to: â€˜InvoicePurchases12312016.csvâ€™\n",
            "\n",
            "\n",
            "          InvoicePu   0%[                    ]       0  --.-KB/s               \n",
            "InvoicePurchases123 100%[===================>] 577.17K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-07-08 21:33:36 (82.3 MB/s) - â€˜InvoicePurchases12312016.csvâ€™ saved [591021/591021]\n",
            "\n",
            "âœ… Downloads completed.\n",
            "\n",
            "== Download and extract ZIPs from the specified URLs ==\n",
            "\n",
            " Downloading ZIP from: https://drive.google.com/file/d/1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY/view?usp=sharing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY\n",
            "From (redirected): https://drive.google.com/uc?id=1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY&confirm=t&uuid=e1caf9ea-2fc7-4216-b5a1-e8ea35041738\n",
            "To: /content/archivo.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150M/150M [00:00<00:00, 239MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unzipping into: archivos/\n",
            " Extracted content: \n",
            "  - SalesFINAL12312016.csv\n",
            "  - PurchasesFINAL12312016.csv\n",
            "\n",
            " Downloading ZIP from: https://drive.google.com/file/d/1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd/view?usp=drive_link\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd\n",
            "From (redirected): https://drive.google.com/uc?id=1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd&confirm=t&uuid=06b0548e-e9b8-4d41-980d-d59318b23035\n",
            "To: /content/archivo.zip\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45.9M/45.9M [00:00<00:00, 170MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Unzipping into: archivos/\n",
            " Extracted content: \n",
            "  - SalesFINAL12312016.csv\n",
            "  - PurchasesFINAL12312016.csv\n",
            "== Read CSV files into pandas DataFrames ==\n",
            "== Info df1 (2017PurchasePricesDec) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12261 entries, 0 to 12260\n",
            "Data columns (total 9 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   Brand           12261 non-null  int64  \n",
            " 1   Description     12260 non-null  object \n",
            " 2   Price           12261 non-null  float64\n",
            " 3   Size            12260 non-null  object \n",
            " 4   Volume          12260 non-null  object \n",
            " 5   Classification  12261 non-null  int64  \n",
            " 6   PurchasePrice   12261 non-null  float64\n",
            " 7   VendorNumber    12261 non-null  int64  \n",
            " 8   VendorName      12261 non-null  object \n",
            "dtypes: float64(2), int64(3), object(4)\n",
            "memory usage: 862.2+ KB\n",
            "\n",
            "== Info df2 (SalesFINAL12312016) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12825363 entries, 0 to 12825362\n",
            "Data columns (total 14 columns):\n",
            " #   Column          Dtype  \n",
            "---  ------          -----  \n",
            " 0   InventoryId     object \n",
            " 1   Store           int64  \n",
            " 2   Brand           int64  \n",
            " 3   Description     object \n",
            " 4   Size            object \n",
            " 5   SalesQuantity   int64  \n",
            " 6   SalesDollars    float64\n",
            " 7   SalesPrice      float64\n",
            " 8   SalesDate       object \n",
            " 9   Volume          float64\n",
            " 10  Classification  int64  \n",
            " 11  ExciseTax       float64\n",
            " 12  VendorNo        int64  \n",
            " 13  VendorName      object \n",
            "dtypes: float64(4), int64(5), object(5)\n",
            "memory usage: 1.3+ GB\n",
            "\n",
            "== Info df3 (BegInvFINAL12312016) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 206529 entries, 0 to 206528\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count   Dtype  \n",
            "---  ------       --------------   -----  \n",
            " 0   InventoryId  206529 non-null  object \n",
            " 1   Store        206529 non-null  int64  \n",
            " 2   City         206529 non-null  object \n",
            " 3   Brand        206529 non-null  int64  \n",
            " 4   Description  206529 non-null  object \n",
            " 5   Size         206529 non-null  object \n",
            " 6   onHand       206529 non-null  int64  \n",
            " 7   Price        206529 non-null  float64\n",
            " 8   startDate    206529 non-null  object \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 14.2+ MB\n",
            "\n",
            "== Info df4 (EndInvFINAL12312016) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 224489 entries, 0 to 224488\n",
            "Data columns (total 9 columns):\n",
            " #   Column       Non-Null Count   Dtype  \n",
            "---  ------       --------------   -----  \n",
            " 0   InventoryId  224489 non-null  object \n",
            " 1   Store        224489 non-null  int64  \n",
            " 2   City         223205 non-null  object \n",
            " 3   Brand        224489 non-null  int64  \n",
            " 4   Description  224489 non-null  object \n",
            " 5   Size         224489 non-null  object \n",
            " 6   onHand       224489 non-null  int64  \n",
            " 7   Price        224489 non-null  float64\n",
            " 8   endDate      224489 non-null  object \n",
            "dtypes: float64(1), int64(3), object(5)\n",
            "memory usage: 15.4+ MB\n",
            "\n",
            "== Info df5 (InvoicePurchases12312016) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5543 entries, 0 to 5542\n",
            "Data columns (total 10 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   VendorNumber  5543 non-null   int64  \n",
            " 1   VendorName    5543 non-null   object \n",
            " 2   InvoiceDate   5543 non-null   object \n",
            " 3   PONumber      5543 non-null   int64  \n",
            " 4   PODate        5543 non-null   object \n",
            " 5   PayDate       5543 non-null   object \n",
            " 6   Quantity      5543 non-null   int64  \n",
            " 7   Dollars       5543 non-null   float64\n",
            " 8   Freight       5543 non-null   float64\n",
            " 9   Approval      374 non-null    object \n",
            "dtypes: float64(2), int64(3), object(5)\n",
            "memory usage: 433.2+ KB\n",
            "\n",
            "== Info df6 (PurchasesFINAL12312016) ==\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2372474 entries, 0 to 2372473\n",
            "Data columns (total 16 columns):\n",
            " #   Column          Dtype  \n",
            "---  ------          -----  \n",
            " 0   InventoryId     object \n",
            " 1   Store           int64  \n",
            " 2   Brand           int64  \n",
            " 3   Description     object \n",
            " 4   Size            object \n",
            " 5   VendorNumber    int64  \n",
            " 6   VendorName      object \n",
            " 7   PONumber        int64  \n",
            " 8   PODate          object \n",
            " 9   ReceivingDate   object \n",
            " 10  InvoiceDate     object \n",
            " 11  PayDate         object \n",
            " 12  PurchasePrice   float64\n",
            " 13  Quantity        int64  \n",
            " 14  Dollars         float64\n",
            " 15  Classification  int64  \n",
            "dtypes: float64(2), int64(6), object(8)\n",
            "memory usage: 289.6+ MB\n",
            "\n",
            "== Extract VAR_tbl_ values and assign to variables ==\n",
            "== Create dictionary of table names and DataFrames ==\n",
            "== Save DataFrames to SQLite if not empty ==\n",
            "Saving PricingPurchasesDec (12261 rows)...\n",
            "Saving BegInvDec (12825363 rows)...\n",
            "Saving EndInvDec (206529 rows)...\n",
            "Saving VendorInvoicesDec (224489 rows)...\n",
            "Saving PurchasesDec (5543 rows)...\n",
            "Saving SalesDec (2372474 rows)...\n",
            "Process BIBITOR completed successfully\n",
            "Updating stage for BIBITOR to 2\n",
            "('BIBITOR', 'VAR_GET_2017PurchasePricesDec', 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1dGNwXDhVuVzvHAz0yWhE99luxT1B1G0d\" -O 2017PurchasePricesDec.csv')\n",
            "('BIBITOR', 'VAR_GET_BegInvFINAL12312016', 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1h5t-HUJvryZiPMRzIdtYXdh4-ofmMCVK\" -O BegInvFINAL12312016.csv')\n",
            "('BIBITOR', 'VAR_GET_EndInvFINAL12312016', 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hzZhhfWr6sIO_lxIN9IkSlARjjNjE6pB\" -O EndInvFINAL12312016.csv')\n",
            "('BIBITOR', 'VAR_GET_InvoicePurchases12312016', 'wget --no-check-certificate \"https://drive.google.com/uc?export=download&id=10zwe070P7gCSQ7cDBkW5nOHTt45o7ya7\" -O InvoicePurchases12312016.csv')\n",
            "('BIBITOR', 'VAR_GET_PurchasesFINAL12312016', 'https://drive.google.com/file/d/1W1VIx6grLFjoqOWvyg66BBGt06Q7X7Xd/view?usp=drive_link')\n",
            "('BIBITOR', 'VAR_GET_SalesFINAL12312016', 'https://drive.google.com/file/d/1APCHTFo79T0bd_ZANbjdwJkS8uV8xDHY/view?usp=sharing')\n",
            "('BIBITOR', 'VAR_tbl_2017PurchasePricesDec', 'PricingPurchasesDec')\n",
            "('BIBITOR', 'VAR_tbl_BegInvFINAL12312016', 'BegInvDec')\n",
            "('BIBITOR', 'VAR_tbl_EndInvFINAL12312016', 'EndInvDec')\n",
            "('BIBITOR', 'VAR_tbl_InvoicePurchases12312016', 'VendorInvoicesDec')\n",
            "('BIBITOR', 'VAR_tbl_PurchasesFINAL12312016', 'PurchasesDec')\n",
            "('BIBITOR', 'VAR_tbl_SalesFINAL12312016', 'SalesDec')\n",
            "('BIBITOR', 'ETAPA', '2')\n",
            "End of block 3 (LOAD CSV DATA SOURCES) STAGE 1 2025-07-08 21:37:58.722196\n",
            "(LOAD CSV DATA SOURCES) STAGE 1 completed in: 1970-01-01 00:04:33.218784\n",
            "Start of block 4 (LOAD TABLES TO DATAFRAMES AND BUSINESS LOGIC) STAGE 2 2025-07-08 21:37:58.722249\n",
            "== Calling function load_tables_from_result to load tables into dfs and then save them into their corresponding dataframes ==\n",
            " Table 'PricingPurchasesDec' loaded and registered.\n",
            " Table 'BegInvDec' loaded and registered.\n",
            " Table 'EndInvDec' loaded and registered.\n",
            " Table 'VendorInvoicesDec' loaded and registered.\n",
            " Table 'PurchasesDec' loaded and registered.\n",
            " Table 'SalesDec' loaded and registered.\n",
            "== Performing printSchema() on dataframe df_SalesDec ==\n",
            "root\n",
            " |-- InventoryId: string (nullable = true)\n",
            " |-- Store: integer (nullable = true)\n",
            " |-- Brand: integer (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Size: string (nullable = true)\n",
            " |-- VendorNumber: integer (nullable = true)\n",
            " |-- VendorName: string (nullable = true)\n",
            " |-- PONumber: integer (nullable = true)\n",
            " |-- PODate: string (nullable = true)\n",
            " |-- ReceivingDate: string (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- PayDate: string (nullable = true)\n",
            " |-- PurchasePrice: double (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Dollars: double (nullable = true)\n",
            " |-- Classification: integer (nullable = true)\n",
            "\n",
            "== Performing printSchema() on dataframe df_PricingPurchasesDec ==\n",
            "root\n",
            " |-- Brand: integer (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Size: string (nullable = true)\n",
            " |-- Volume: string (nullable = true)\n",
            " |-- Classification: integer (nullable = true)\n",
            " |-- PurchasePrice: double (nullable = true)\n",
            " |-- VendorNumber: integer (nullable = true)\n",
            " |-- VendorName: string (nullable = true)\n",
            "\n",
            "== Performing printSchema() on dataframe df_BegInvDec and caching it because it's heavy and we need it in memory ==\n",
            "root\n",
            " |-- InventoryId: string (nullable = true)\n",
            " |-- Store: integer (nullable = true)\n",
            " |-- Brand: integer (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Size: string (nullable = true)\n",
            " |-- SalesQuantity: integer (nullable = true)\n",
            " |-- SalesDollars: double (nullable = true)\n",
            " |-- SalesPrice: double (nullable = true)\n",
            " |-- SalesDate: string (nullable = true)\n",
            " |-- Volume: double (nullable = true)\n",
            " |-- Classification: integer (nullable = true)\n",
            " |-- ExciseTax: double (nullable = true)\n",
            " |-- VendorNo: integer (nullable = true)\n",
            " |-- VendorName: string (nullable = true)\n",
            "\n",
            "== Performing printSchema() on dataframe df_EndInvDec ==\n",
            "root\n",
            " |-- InventoryId: string (nullable = true)\n",
            " |-- Store: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Brand: integer (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Size: string (nullable = true)\n",
            " |-- onHand: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- startDate: string (nullable = true)\n",
            "\n",
            "== Performing printSchema() on dataframe df_VendorInvoicesDec ==\n",
            "root\n",
            " |-- InventoryId: string (nullable = true)\n",
            " |-- Store: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Brand: integer (nullable = true)\n",
            " |-- Description: string (nullable = true)\n",
            " |-- Size: string (nullable = true)\n",
            " |-- onHand: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- endDate: string (nullable = true)\n",
            "\n",
            "== Performing printSchema() on dataframe df_PurchasesDec ==\n",
            "root\n",
            " |-- VendorNumber: integer (nullable = true)\n",
            " |-- VendorName: string (nullable = true)\n",
            " |-- InvoiceDate: string (nullable = true)\n",
            " |-- PONumber: integer (nullable = true)\n",
            " |-- PODate: string (nullable = true)\n",
            " |-- PayDate: string (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Dollars: double (nullable = true)\n",
            " |-- Freight: double (nullable = true)\n",
            " |-- Approval: string (nullable = true)\n",
            "\n",
            "== Selecting necessary columns from Pricing dataframe ==\n",
            "== Joining sales dataframe with cost info from pricing dataframe ==\n",
            "== Calculating Profit ($), Revenue ($), and Margin (%) ==\n",
            "== Getting Top 10 products by Profit ==\n",
            "+-----+--------------------+-----+--------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-----------------+------------------+------------------+\n",
            "|Brand|         Description| Size|         InventoryId|Store|SalesQuantity|SalesDollars|SalesPrice| SalesDate|Volume|Classification|ExciseTax|VendorNo|          VendorName|PurchasePrice|           Profit|           Revenue|            Margin|\n",
            "+-----+--------------------+-----+--------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-----------------+------------------+------------------+\n",
            "|35989|    Ch Haut Brion 10|750mL|  69_MOUNTMEND_35989|   69|           12|    15599.88|   1299.99|2016-07-25| 750.0|             2|     1.35|   10754|PERFECTA WINES   ...|       844.15|          5470.08|15599.880000000001|35.064885114500875|\n",
            "|42188|Moet & Chandon Ne...|750mL|  32_MOUNTMEND_42188|   32|          276|    14487.24|     52.49|2016-02-04| 750.0|             2|    31.05|    8112|MOET HENNESSY USA...|        34.01|5100.480000000001|          14487.24| 35.20670603924558|\n",
            "|42188|Moet & Chandon Ne...|750mL|  32_MOUNTMEND_42188|   32|          247|    13335.53|     53.99|2016-04-26| 750.0|             2|    27.79|    8112|MOET HENNESSY USA...|        34.01|4935.060000000001|          13335.53| 37.00685312094833|\n",
            "|42188|Moet & Chandon Ne...|750mL|   15_WANBORNE_42188|   15|          253|    13279.97|     52.49|2016-02-06| 750.0|             2|    28.46|    8112|MOET HENNESSY USA...|        34.01|4675.440000000001|13279.970000000001|35.206706039245574|\n",
            "|42188|Moet & Chandon Ne...|750mL|59_CLAETHORPES_42188|   59|          222|    11985.78|     53.99|2016-10-20| 750.0|             2|    24.97|    8112|MOET HENNESSY USA...|        34.01|4435.560000000001|          11985.78|37.006853120948335|\n",
            "|42188|Moet & Chandon Ne...|750mL|59_CLAETHORPES_42188|   59|          210|     11547.9|     54.99|2016-09-18| 750.0|             2|    23.62|    8112|MOET HENNESSY USA...|        34.01|4405.800000000001|           11547.9|38.152391343880716|\n",
            "| 3858|    Grey Goose Vodka|750mL|     33_HORNSEY_3858|   33|          807|    18552.93|     22.99|2016-07-25| 750.0|             1|   635.51|     480|BACARDI USA INC  ...|        17.77|4212.539999999999|          18552.93|22.705524140930834|\n",
            "|42188|Moet & Chandon Ne...|750mL| 63_SWORDBREAK_42188|   63|          210|     11337.9|     53.99|2016-04-30| 750.0|             2|    23.62|    8112|MOET HENNESSY USA...|        34.01|4195.800000000001|           11337.9|37.006853120948335|\n",
            "|42188|Moet & Chandon Ne...|750mL|59_CLAETHORPES_42188|   59|          210|     11337.9|     53.99|2016-10-27| 750.0|             2|    23.62|    8112|MOET HENNESSY USA...|        34.01|4195.800000000001|           11337.9|37.006853120948335|\n",
            "|36063|Ch La Mission Hau...|750mL|  69_MOUNTMEND_36063|   69|           12|    13199.88|   1099.99|2016-07-25| 750.0|             2|     1.35|   10754|PERFECTA WINES   ...|       753.42|          4158.84|13199.880000000001|31.506650060455094|\n",
            "+-----+--------------------+-----+--------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-----------------+------------------+------------------+\n",
            "\n",
            "== Getting Top 10 products by Margin ==\n",
            "+-----+--------------------+-----+-----------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+------+-------+------+\n",
            "|Brand|         Description| Size|      InventoryId|Store|SalesQuantity|SalesDollars|SalesPrice| SalesDate|Volume|Classification|ExciseTax|VendorNo|          VendorName|PurchasePrice|Profit|Revenue|Margin|\n",
            "+-----+--------------------+-----+-----------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+------+-------+------+\n",
            "| 2166|The Macallan Doub...|750mL| 30_CULCHETH_2166|   30|            1|       65.99|     65.99|2016-08-26| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|  2_ASHBORNE_2166|    2|            1|       65.99|     65.99|2016-08-29| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|38_GOULCREST_2166|   38|            1|       65.99|     65.99|2016-08-24| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|32_MOUNTMEND_2166|   32|            1|       65.99|     65.99|2016-08-16| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|23_ARBINGTON_2166|   23|            1|       65.99|     65.99|2016-08-13| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|32_MOUNTMEND_2166|   32|            1|       65.99|     65.99|2016-08-21| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|32_MOUNTMEND_2166|   32|            1|       65.99|     65.99|2016-08-23| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|32_MOUNTMEND_2166|   32|            1|       65.99|     65.99|2016-08-28| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|  10_HORNSEY_2166|   10|            1|       65.99|     65.99|2016-08-31| 750.0|             1|     0.79|    2561|EDRINGTON AMERICA...|          0.0| 65.99|  65.99| 100.0|\n",
            "| 2166|The Macallan Doub...|750mL|38_GOULCREST_2166|   38|            2|      131.98|     65.99|2016-08-17| 750.0|             1|     1.57|    2561|EDRINGTON AMERICA...|          0.0|131.98| 131.98| 100.0|\n",
            "+-----+--------------------+-----+-----------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+------+-------+------+\n",
            "\n",
            "== Aggregating Profit and Revenue by Brand ==\n",
            "== Getting Top 10 brands by Profit ==\n",
            "+-----+------------------+------------------+------------------+\n",
            "|Brand|       TotalProfit|      TotalRevenue|            Margin|\n",
            "+-----+------------------+------------------+------------------+\n",
            "| 6570| 875616.5199999306| 2326007.779999907| 37.64460839421464|\n",
            "| 3858| 863060.1999998721|3383912.4000005093|25.504803256719715|\n",
            "| 2585| 650689.7900000663|2148965.5299997157|30.279210202136298|\n",
            "| 3650| 601694.0199999507|2009508.6099997887| 29.94234595491551|\n",
            "| 6654|  537302.209999979|1584665.4099999077| 33.90635061568046|\n",
            "|42188| 530407.5399999982| 1533634.519999969| 34.58500269021128|\n",
            "|11219|433416.55999998754|1095045.0399999167|39.579792991895616|\n",
            "| 1232| 407617.3599999394|1255188.9399997946| 32.47458187450298|\n",
            "| 3505|   383707.69000001| 1355204.969999795|28.313627716408686|\n",
            "| 8512|379818.93999998126|1456174.7799997765| 26.08333458433057|\n",
            "+-----+------------------+------------------+------------------+\n",
            "\n",
            "== Getting Top 10 brands by Margin ==\n",
            "+-----+------------------+------------------+-----------------+\n",
            "|Brand|       TotalProfit|      TotalRevenue|           Margin|\n",
            "+-----+------------------+------------------+-----------------+\n",
            "| 2166| 98245.68000000085| 98245.68000000085|            100.0|\n",
            "|18627|            257.06|            308.23|83.39876066573663|\n",
            "| 1771| 52951.36000000107| 70680.61000000162|74.91638795986603|\n",
            "|12218| 752.4299999999992|1035.5700000000008|72.65853587879126|\n",
            "|13583|            697.68|            970.92|71.85761957730811|\n",
            "|22567|412.55999999999983| 579.4800000000002|71.19486436115132|\n",
            "|15860| 5437.039999999982| 7836.079999999975|69.38469234617307|\n",
            "|13688| 785.1999999999999|           1137.16|69.04921031341235|\n",
            "|23137|51.019999999999996|             74.99|68.03573809841312|\n",
            "|44278|            532.64| 787.5800000000002|67.62995505218517|\n",
            "+-----+------------------+------------------+-----------------+\n",
            "\n",
            "== Filtering products losing money (Profit < 0) ==\n",
            "+-----+------------------+-----+-------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-------------------+-------+-------------------+\n",
            "|Brand|       Description| Size|        InventoryId|Store|SalesQuantity|SalesDollars|SalesPrice| SalesDate|Volume|Classification|ExciseTax|VendorNo|          VendorName|PurchasePrice|             Profit|Revenue|             Margin|\n",
            "+-----+------------------+-----+-------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-------------------+-------+-------------------+\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            3|        5.37|      1.79|2016-03-19| 187.0|             2|     0.08|    2000|SOUTHERN WINE & S...|         2.05|-0.7799999999999994|   5.37|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-03-20| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            6|       10.74|      1.79|2016-03-21| 187.0|             2|     0.17|    2000|SOUTHERN WINE & S...|         2.05|-1.5599999999999987|  10.74|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-03-23| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-03-24| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            9|       16.11|      1.79|2016-03-25| 187.0|             2|     0.25|    2000|SOUTHERN WINE & S...|         2.05| -2.339999999999998|  16.11|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-03-28| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            1|        1.79|      1.79|2016-03-30| 187.0|             2|     0.03|    2000|SOUTHERN WINE & S...|         2.05|-0.2599999999999998|   1.79|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            4|        7.16|      1.79|2016-04-01| 187.0|             2|     0.11|    2000|SOUTHERN WINE & S...|         2.05|-1.0399999999999991|   7.16|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|           13|       23.27|      1.79|2016-04-02| 187.0|             2|     0.36|    2000|SOUTHERN WINE & S...|         2.05|-3.3799999999999972|  23.27|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            4|        7.16|      1.79|2016-04-03| 187.0|             2|     0.11|    2000|SOUTHERN WINE & S...|         2.05|-1.0399999999999991|   7.16|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-04-10| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            3|        5.37|      1.79|2016-04-11| 187.0|             2|     0.08|    2000|SOUTHERN WINE & S...|         2.05|-0.7799999999999994|   5.37|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            2|        3.58|      1.79|2016-04-13| 187.0|             2|     0.06|    2000|SOUTHERN WINE & S...|         2.05|-0.5199999999999996|   3.58|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            1|        1.79|      1.79|2016-04-14| 187.0|             2|     0.03|    2000|SOUTHERN WINE & S...|         2.05|-0.2599999999999998|   1.79|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            6|       10.74|      1.79|2016-04-16| 187.0|             2|     0.17|    2000|SOUTHERN WINE & S...|         2.05|-1.5599999999999987|  10.74|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            6|       10.74|      1.79|2016-04-17| 187.0|             2|     0.17|    2000|SOUTHERN WINE & S...|         2.05|-1.5599999999999987|  10.74|-14.525139664804456|\n",
            "|24478|     ZIPZ Cab Svgn|187mL|67_EANVERNESS_24478|   67|            4|        7.16|      1.79|2016-04-20| 187.0|             2|     0.11|    2000|SOUTHERN WINE & S...|         2.05|-1.0399999999999991|   7.16|-14.525139664804456|\n",
            "|44714|Buehler Znfdl Napa|750mL|   33_HORNSEY_44714|   33|            1|       11.99|     11.99|2016-05-29| 750.0|             2|     0.11|    4425|MARTIGNETTI COMPA...|        13.51|-1.5199999999999996|  11.99|-12.677231025854876|\n",
            "|44714|Buehler Znfdl Napa|750mL| 34_PITMERDEN_44714|   34|            2|       23.98|     11.99|2016-05-31| 750.0|             2|     0.22|    4425|MARTIGNETTI COMPA...|        13.51| -3.039999999999999|  23.98|-12.677231025854876|\n",
            "+-----+------------------+-----+-------------------+-----+-------------+------------+----------+----------+------+--------------+---------+--------+--------------------+-------------+-------------------+-------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "== Filtering brands losing money (TotalProfit < 0) ==\n",
            "+-----+-------------------+------------------+-------------------+\n",
            "|Brand|        TotalProfit|      TotalRevenue|             Margin|\n",
            "+-----+-------------------+------------------+-------------------+\n",
            "|19833| -666.3800000000001|1741.4100000000003|  -38.2666919335481|\n",
            "|12401|-1112.2400000000014|8145.5399999999645| -13.65458889158983|\n",
            "|19996| -2681.379999999948|36152.530000000355| -7.416852983732872|\n",
            "|46929| -7.579999999999991|114.82000000000001| -6.601637345410199|\n",
            "| 8090| -95.46000000000002|           1109.63| -8.602867622540849|\n",
            "|33331|-12576.639999999676| 65065.64000000019| -19.32915744777065|\n",
            "| 4876|-1132.6000000000238|16643.099999999777|-6.8052225847350485|\n",
            "|26710|-20221.650000000107|26032.050000000036| -77.67982160452243|\n",
            "| 3963|  -742.080000000001|19342.400000000016|-3.8365456199851122|\n",
            "|21505| -7942.080000000082| 23891.88000000014|  -33.2417541022307|\n",
            "|22995|            -554.75| 4705.049999999991|-11.790522948746581|\n",
            "|18947|-214.51999999999998|1111.4800000000002| -19.30039226976643|\n",
            "|47049|-115.50000000000006| 794.7000000000002| -14.53378633446584|\n",
            "|25827| -4125.710000000003|18956.129999999997|-21.764516280485537|\n",
            "|10666|           -12434.0|31188.640000000032| -39.86707980854563|\n",
            "|25588| -24762.39000000058| 55406.43000000014| -44.69226766640716|\n",
            "| 7680|-1149.7600000000068|          20576.96| -5.587608665225606|\n",
            "|24478|-20.279999999999983|            139.62|-14.525139664804456|\n",
            "|25797| -6136.199999999993|24941.800000000014|-24.602073627404557|\n",
            "|44714|-14470.000000000853|119616.75000000349|-12.096968024963418|\n",
            "+-----+-------------------+------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "== Saving reports using save_csv function ==\n",
            "End of blocks 4, 5 and 6 (LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2  2025-07-08 21:47:09.263290\n",
            "(LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 completed in : 1970-01-01 00:09:10.541040\n",
            "== We call the save_csv function to save the dfs in our path /content/final_reports/....... to be able to download them and use as sources ==\n",
            "End of blocks 4, 5 and 6 (LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2  2025-07-08 21:47:09.263290\n",
            "(LOAD TABLES TO DATAFRAMES, BUSINESS LOGIC AND REPORT SAVING) STAGE 2 completed in : 1970-01-01 00:09:10.541040\n",
            "== Closing SQLite connection if open ==\n",
            "SQLite connection closed.\n",
            "== Releasing memory by deleting used variables ==\n",
            "End of process: 2025-07-08 21:49:21.430996\n",
            "Process completed in 955.95 seconds\n",
            "== Closing spark session to free resources ==\n"
          ]
        }
      ]
    }
  ]
}